{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 2: Approximate Variational Inference\n",
    "\n",
    "During this session, we will first continue the regression experiments started last week by introducing MCDropout variational inference on the sinusoidal toy dataset. Then, we will explore and compare variational approaches, including 'brute-force' mean-field approximation with Pyro library and MCDropout approximation with Logistic Regression on a 2D binary classification dataset.\n",
    "\n",
    "**Goal**: Take hand on approximate variational inference methods and understand how it works\n",
    "\n",
    "**Requirements**: \n",
    "- Pytorch library:\n",
    "`pip install torch torchvision`\n",
    "- Pyro library:\n",
    "`pip install pyro-ppl`\n",
    "\n",
    "NB: if you don't have root access on the computer, add `--user` to install the library locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: MC Dropout variational inference in regression\n",
    "\n",
    "We expand last week's analysis on non-linear models for Bayesian regression. Here, we will define and use a Bayesian Neural Network with MCDropout variational inference to estimate uncertainty on a sinusoidal toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sinusoidal toy dataset\n",
    "def f_sinus(x, noise_amount, sigma=0.2):\n",
    "    y = np.sin(2*np.pi*x) + x\n",
    "    noise = np.random.normal(0, sigma, len(x))\n",
    "    return y + noise_amount * noise\n",
    "\n",
    "sigma = 0.1\n",
    "nbpoints = 50\n",
    "\n",
    "# Create training and test points\n",
    "dataset_sinus = {}\n",
    "dataset_sinus['X_train'] = np.random.uniform(0, 1, nbpoints)\n",
    "dataset_sinus['y_train'] = f_sinus(dataset_sinus['X_train'], noise_amount=1)\n",
    "dataset_sinus['X_test'] = np.linspace(-0.5,1.5, 10*nbpoints)\n",
    "dataset_sinus['y_test'] = f_sinus(dataset_sinus['X_test'], noise_amount=0)\n",
    "\n",
    "# Hyperparameters\n",
    "dataset_sinus['ALPHA'] = 0.01\n",
    "dataset_sinus['BETA'] = 1/(2.0*sigma**2)\n",
    "\n",
    "# Plot dataset\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.xlim(xmin =-0.5, xmax = 1.5)\n",
    "plt.ylim(ymin = -2, ymax = 3)\n",
    "plt.plot(dataset_sinus['X_test'], dataset_sinus['y_test'], color='green', linewidth=2,\n",
    "         label=\"Ground Truth\")\n",
    "plt.plot(dataset_sinus['X_train'], dataset_sinus['y_train'], 'o', color='blue', label='Training points')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(dataset_sinus['X_train'].copy()).float().unsqueeze(dim=1)\n",
    "y_train_tensor = torch.from_numpy(dataset_sinus['y_train'].copy()).float()\n",
    "X_test_tensor = torch.from_numpy(dataset_sinus['X_test'].copy()).float().unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function: plot results\n",
    "def plot_results(X_train, y_train, X_test, y_test, y_pred, std_pred,\n",
    "                 xmin=-2, xmax=2, ymin=-2, ymax=1, stdmin=0.30, stdmax=0.45):\n",
    "    \"\"\"Given a dataset and predictions on test set, this function draw 2 subplots:\n",
    "    - left plot compares train set, ground-truth (test set) and predictions\n",
    "    - right plot represents the predictive variance over input range\n",
    "    \n",
    "    Args:\n",
    "      X_train: (array) train inputs, sized [N,]\n",
    "      y_train: (array) train labels, sized [N, ]\n",
    "      X_test: (array) test inputs, sized [N,]\n",
    "      y_test: (array) test labels, sized [N, ]\n",
    "      y_pred: (array) mean prediction, sized [N, ]\n",
    "      std_pred: (array) std prediction, sized [N, ]\n",
    "      xmin: (float) min value for x-axis on left and right plot\n",
    "      xmax: (float) max value for x-axis on left and right plot\n",
    "      ymin: (float) min value for y-axis on left plot\n",
    "      ymax: (float) max value for y-axis on left plot\n",
    "      stdmin: (float) min value for y-axis on right plot\n",
    "      stdmax: (float) max value for y-axis on right plot\n",
    "      \n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(121)\n",
    "    plt.xlim(xmin = xmin, xmax = xmax)\n",
    "    plt.ylim(ymin = ymin, ymax = ymax)\n",
    "    plt.plot(X_test, y_test, color='green', linewidth=2,\n",
    "             label=\"Ground Truth\")\n",
    "    plt.plot(X_train, y_train, 'o', color='blue', label='Training points')\n",
    "    plt.plot(X_test, y_pred, color='red', label=\"MC dropout\")\n",
    "    plt.fill_between(X_test, y_pred-std_pred, y_pred+std_pred, color='indianred', label='1 std. int.')\n",
    "    plt.fill_between(X_test, y_pred-std_pred*2, y_pred-std_pred, color='lightcoral')\n",
    "    plt.fill_between(X_test, y_pred+std_pred*1, y_pred+std_pred*2, color='lightcoral', label='2 std. int.')\n",
    "    plt.fill_between(X_test, y_pred-std_pred*3, y_pred-std_pred*2, color='mistyrose')\n",
    "    plt.fill_between(X_test, y_pred+std_pred*2, y_pred+std_pred*3, color='mistyrose', label='3 std. int.')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.title(\"Predictive variance along x-axis\")\n",
    "    plt.xlim(xmin = xmin, xmax = xmax)\n",
    "    plt.ylim(ymin = stdmin, ymax = stdmax)\n",
    "    plt.plot(X_test, std_pred**2, color='red', label=\"\\u03C3Â² {}\".format(\"Pred\"))\n",
    "\n",
    "    # Get training domain\n",
    "    training_domain = []\n",
    "    current_min = sorted(X_train)[0]\n",
    "    for i, elem in enumerate(sorted(X_train)):\n",
    "        if elem-sorted(X_train)[i-1]>1:\n",
    "            training_domain.append([current_min,sorted(X_train)[i-1]])\n",
    "            current_min = elem\n",
    "    training_domain.append([current_min, sorted(X_train)[-1]])\n",
    "    \n",
    "    # Plot domain\n",
    "    for j, (min_domain, max_domain) in enumerate(training_domain):\n",
    "        plt.axvspan(min_domain, max_domain, alpha=0.5, color='gray', label=\"Training area\" if j==0 else '')\n",
    "    plt.axvline(X_train.mean(), linestyle='--', label=\"Training barycentre\")   \n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function: plot and show learning process in regression\n",
    "def plot_learning_process(X_train_tensor, y_train_tensor, outputs, loss, epoch):\n",
    "    \"\"\"Given 1D training input and prediction vector, this function output a plot of \n",
    "    the predicted function and value of the loss, both updated at each epoch.\n",
    "\n",
    "    Args:\n",
    "      X_train_tensor: (tensor) train inputs, sized [N,1]\n",
    "      y_train_tensor: (tensor) train labels, sized [N, ]\n",
    "      output: (tensor) prediction vector from NN, sized [N, ]\n",
    "      loss: (float) current loss value\n",
    "      epoch: (int) current epoch\n",
    "      \n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    plt.cla()\n",
    "    ax.set_title('Regression Analysis')\n",
    "    ax.set_xlim(-0.5, 1.5)\n",
    "    ax.set_ylim(-2, 3)\n",
    "    ax.scatter(X_train_tensor.data.numpy(), y_train_tensor.data.numpy(), color = \"orange\")\n",
    "    ax.plot(X_train_tensor.sort(0).values.data.numpy(), output[X_train_tensor.sort(0).indices].data.numpy(), 'g-', lw=3)\n",
    "    ax.text(1.0, 0.3, 'Step = %d' % epoch, fontdict={'size': 20, 'color':  'red'})\n",
    "    ax.text(1.0, 0, 'Loss = %.4f' % loss, fontdict={'size': 20, 'color':  'red'})\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1: Code a Multi-Layer Perceptron, a neural network with one hidden layer, and add a Dropout Layer with p=0.25.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Code MLP with 1 hidden layer and a dropout layer. Be careful, the dropout layer should be also activated \n",
    "#during test time.\n",
    "#(Hint: we may want to look out at F.dropout())\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Pytorch MLP model with an added dropout layer\"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        # To DO\n",
    "\n",
    "    def forward(self, x):\n",
    "        # To DO\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(hidden_size=100)\n",
    "net.train()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2: Train the model for 5000 epoch, batch size equals to training set. Use ``plot_learning_process()`` to visualize training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Train MLP for 5000 epochs\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,7))\n",
    "\n",
    "# To DO here\n",
    "\n",
    "\n",
    "# For plotting and showing learning process at each epoch, uncomment and indent line below\n",
    "#if (epoch+1)%100==0:\n",
    "#    plot_learning_process(X_train_tensor, y_train_tensor, output, loss.data.numpy(), epoch+1)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3: Predict on test set using s=1000 samples and visualize results. What can you say about the predictive variance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Predict on test set and visualize results using plot_results\n",
    "\n",
    "net.eval()\n",
    "net.training = True\n",
    "\n",
    "# To DO here\n",
    "\n",
    "\n",
    "# For plotting using, uncomment the following function (need to have compute y_pred and std_pred)\n",
    "#plot_results(dataset_sinus['X_train'], dataset_sinus['y_train'], dataset_sinus['X_test'], dataset_sinus['y_test'],\n",
    "#             y_pred, std_pred, xmin=-0.5, xmax=1.5, ymin=-2, ymax=3, stdmin=0, stdmax=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Bayesian Logistic Regression\n",
    "\n",
    "In previous linear regression, our model prediction is of the continuous form $f(x)=wTx+b$.\n",
    "\n",
    "For classification, we wish to predict discrete class labels $\\mathcal{C}_k$ to a sample $x$. \n",
    "Let's consider here binary classification:\n",
    "$$f(x) = \\sigma(w^T x + b)$$\n",
    "where $\\sigma(t)= \\frac{1}{1+\\exp(t)}$ is the sigmoid function.\n",
    "\n",
    "As in linear regression, we define a Gaussian prior: \n",
    "$$ p(w) = \\mathcal{N}(w \\vert \\mu_0, \\Sigma_0) $$\n",
    "Unfortunately, the posterior distribution isn't tractable as the likelihood isn't conjugate to the prior anymore.\n",
    "\n",
    "We will explore in the following different methods to obtain an estimate of the posterior distribution and hence the predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy dataset\n",
    "X, y = make_blobs(n_samples=100, centers=[(-2,-2),(2,2)], cluster_std=0.80, n_features=2)\n",
    "X = torch.from_numpy(X)\n",
    "y = torch.from_numpy(y.astype(float))\n",
    "torch_train_dataset = data.TensorDataset(X,y) # create your datset\n",
    "train_dataloader = data.DataLoader(torch_train_dataset, batch_size=len(torch_train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='Paired_r', edgecolors='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function: plot and show learning process in classification\n",
    "def plot_decision_boundary(model, X, Y, epoch, accuracy, model_type='classic', samples=100, nbh=2, cmap='RdBu'):    \n",
    "    h = 0.02*nbh\n",
    "    x_min, x_max = X[:,0].min() - 10*h, X[:,0].max() + 10*h\n",
    "    y_min, y_max = X[:,1].min() - 10*h, X[:,1].max() + 10*h\n",
    "    xx, yy = np.meshgrid(np.arange(x_min*2, x_max*2, h),\n",
    "                         np.arange(y_min*2, y_max*2, h))\n",
    "    \n",
    "    test_tensor = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()])\n",
    "    if model_type=='classic':\n",
    "        model.eval()\n",
    "        pred = model(test_tensor)\n",
    "    elif model_type=='svi':\n",
    "        pred = model.forward(test_tensor, n_samples=samples).mean(0)\n",
    "    elif model_type=='mcdropout':\n",
    "        model.eval()\n",
    "        model.training = True\n",
    "        outputs = torch.zeros(samples, test_tensor.shape[0], 1)\n",
    "        for i in range(samples):\n",
    "            outputs[i] = model(test_tensor)\n",
    "        pred = outputs.mean(0).squeeze()\n",
    "    Z = pred.reshape(xx.shape).detach().numpy()\n",
    "\n",
    "    plt.cla()\n",
    "    ax.set_title('Classification Analysis')\n",
    "    ax.contourf(xx, yy, Z, cmap=cmap, alpha=0.25)\n",
    "    ax.contour(xx, yy, Z, colors='k', linestyles=':', linewidths=0.7)\n",
    "    ax.scatter(X[:,0], X[:,1], c=Y, cmap='Paired_r', edgecolors='k');\n",
    "    ax.text(-4, -7, f'Epoch = {epoch+1}, Accuracy = {accuracy:.2%}', fontdict={'size': 12, 'fontweight': 'bold'})\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1 First solution: MAP estimate\n",
    "\n",
    "Here, we reduce our posterior distribution $p(w | D)$ to a point estimate $w_{MAP}$ in order to be able to approximate the predictive distribution as:\n",
    "$$ p(y^* = 1|x^*,D) = \\int p(y^* =1 |x^*,w)p(w | D)dw \\approx p(y^* =1 |x^*,w_{MAP}) $$\n",
    "This approximation is called the **plug-in approximation**.\n",
    "\n",
    "The point estimate corresponds to the Maximum A Posteriori given by:\n",
    "$$ w_{MAP} = arg \\max_w p(w \\vert Y,X) = arg \\max_w p(Y \\vert X,w)p(w) $$\n",
    "In case of a Gaussian prior, we further obtain:\n",
    "$$ w_{MAP} = arg \\min_w \\sum_{n=1}^N \\big ( -y_n \\log \\sigma(w^T x_n + b) - (1-y_n) \\log (1 - \\sigma(w^T x_n + b)) + \\frac{1}{2 \\Sigma_0^2} \\vert \\vert w \\vert \\vert_2^2 \\big ) $$\n",
    "\n",
    "Note that:\n",
    "- We recover our usual **cross-entropy** in classification with a weight decay regularization\n",
    "- Unlike in linear regression $w_{MAP}$ **cannot be computed analytically**\n",
    "- Optimization methods can however be used to compute it, e.g. **stochastic gradient descent**\n",
    "- The main disadvantage is we cannot obtain a distribution over w\n",
    "\n",
    "\n",
    "As a consequence, **the objective is simply to implement and train a Logistic Regression model** with Pytorch and then compute $p(y^* = 1|x^*,D)$ as in a deterministic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1: Code Logistic Regression model class in Pytorch below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Code Logistic Regression in Pytorch\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    \"\"\" A D-dimension Logistic Regression Model in Pytorch\"\"\"\n",
    "    def __init__(self, D):\n",
    "        super().__init__()\n",
    "        # To Do\n",
    "\n",
    "    def forward(self, x):\n",
    "        # To Do\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LogisticRegression(D=2)\n",
    "net.train()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.1, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2:  Train Logistic Regression with gradient descent for 20 epochs in code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Train Logistic Regression with gradient descent for 20 epochs\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "# To DO here\n",
    "\n",
    "# For plotting and showing learning process at each epoch, uncomment and indent line below\n",
    "#plot_decision_boundary(net, X, y, epoch, ((output.squeeze()>=0.5) == y).float().mean(), model_type='classic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3: Analyze the results showed on plot. Looking at $p(y=1 | x, w_{MAP})$, what can you say about points far from train distribution?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2 Second solution: Variational Inference\n",
    "\n",
    "Here, we define an approximating variational distribution $q_\\theta(w)$ parametrized by $\\theta$ and minimize its KL divergence with the unknown true posterior $p(w \\vert D)$. This is equivalent to maximizing the **evidence lower bound (ELBO)** w.r.t to $q_\\theta(w)$:\n",
    "$$ L_{VI}(\\theta) = \\sum_i \\int q_\\theta (w) \\log p(y_i \\vert x_i, w) dw - KL(q_\\theta(w) \\vert\\vert p(w)) $$\n",
    "\n",
    "As we chose a Gaussian prior $p(w)$, we also define $q_\\theta(w)$ as a Gaussian distribution.\n",
    "\n",
    "The predictive distribution can then be approximated using **Monte Carlo sampling**:\n",
    "$$ p(y^*=1|x^*,D) \\approx \\int p(y^*=1|x^*,w)q_\\theta^*(w) \\approx \\frac{1}{S} \\sum_{s=1}^S p(y^*=1|x^*,w_s) $$\n",
    "where $w_s \\sim q^*_\\theta(w)$ are samples from the optimum variational distribution.\n",
    "\n",
    "In this part, we use [Pyro](https://pyro.ai/) library to construct our VI approach. Pyro is a tool for deep probabilistic modeling and supported by PyTorch on the backend.\n",
    "\n",
    "Remind that we defined our Logistic regression model as $f(x) = \\sigma(w^T x + b)$ where $\\sigma(t)= \\frac{1}{1+\\exp(t)}$ is the sigmoid function. As such, we need to place Gaussian distributions on parameters $w$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro import poutine\n",
    "import pyro.optim as pyroopt\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from torch.distributions import constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4: Implement Bayesian Logistic Regression model with variational inference in Pyro by completing the code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Complete code in guide() method below\n",
    "\n",
    "class BayesianLogRegModel(nn.Module):\n",
    "    ''' Pyro model class that contains model and guide definition which will be pass to pyro.infer.SVI() function.\n",
    "    We also define infer_parameters() to train the SVI model and forward() to compute predictions\n",
    "    '''\n",
    "    def __init__(self, D):\n",
    "        ''' Initialize hyparameters for priors and for variational parameters '''\n",
    "        super(BayesianLogRegModel, self).__init__()\n",
    "        # hyperparameters for normal priors\n",
    "        self.w_0_prior_loc = torch.zeros(1, D)\n",
    "        self.w_0_prior_scale = torch.ones(1, D)\n",
    "        self.b_0_prior_loc = torch.zeros(1)\n",
    "        self.b_0_prior_scale = torch.ones(1)\n",
    "        \n",
    "        # initial values of variational parameters\n",
    "        self.w_0_loc = torch.zeros((1, D)).double()\n",
    "        self.w_0_scale = torch.ones((1, D)).double()\n",
    "        self.b_0_loc = torch.zeros((1,)).double()\n",
    "        self.b_0_scale = torch.ones((1,)).double()\n",
    "\n",
    "    def model(self, x, y):\n",
    "        ''' We define the stochastic programming (graph) of our model. It contains pyro.sampling statements \n",
    "        to represent random variables'''\n",
    "        # Sample from prior\n",
    "        w = pyro.sample(\"weight\", dist.Normal(self.w_0_prior_loc, self.w_0_prior_scale).independent(1))\n",
    "        b = pyro.sample(\"bias\", dist.Normal(self.b_0_prior_loc, self.b_0_prior_scale).independent(1))\n",
    "\n",
    "        with pyro.iarange(\"data\", x.size(0)):\n",
    "            # Define model graph\n",
    "            model_logits = (torch.matmul(x, w.T) + b).squeeze()  \n",
    "            # Sample output logits\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(logits=model_logits), obs=y.squeeze())\n",
    "            \n",
    "    def guide(self, x, y=None):\n",
    "        ''' Encoded as a stochastic function, the guide serves to define the variational \n",
    "        posterior distribution. It contains pyro.param statements which represents our learnable theta parameters \n",
    "        and pyro.sampling statements for random variables in a mirror way of the model. These names will be used to align the random \n",
    "        variables in the model and guide'''\n",
    "        # Set-up weight (w) parameters to be optimized to approximate the true posterior\n",
    "        w_loc = pyro.param(\"w_loc\", self.w_0_loc)\n",
    "        w_scale = pyro.param(\"w_scale\", self.w_0_scale, constraint=constraints.positive)\n",
    "\n",
    "        # ===> TO DO here: Set-up bias (b) parameters to be optimized to approximate the true posterior\n",
    "        \n",
    "\n",
    "        # As in model definition, sample w values using the variational parameters \n",
    "        w = pyro.sample(\"weight\", dist.Normal(w_loc, w_scale).independent(1))\n",
    "\n",
    "        # ==> TO DO here : sample b values using the variational parameters \n",
    "        \n",
    "        \n",
    "    def infer_parameters(self, loader, lr=0.1, momentum=0.9, num_epochs=30):\n",
    "        ''' Learning method\n",
    "        Given a training dataset (loader), we set-up an optimizer and a loss (Trace_Elbo) which are\n",
    "        fed into the SVI() function along with previously defined model and guide. Training is compute\n",
    "        as in pytorch, inference and backpropagation steps managed by svi.step()'''\n",
    "        optim = pyroopt.Adam({'lr': lr})\n",
    "        elbo = Trace_ELBO()\n",
    "        svi = SVI(self.model, self.guide, optim, elbo)\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0.0 \n",
    "            total = 0.0\n",
    "            correct = 0.0\n",
    "            for batch_x, batch_y in loader:\n",
    "                # Learning step\n",
    "                loss = svi.step(batch_x, batch_y)\n",
    "                \n",
    "                # Computing prediction for visualization purpose\n",
    "                pred = self.forward(batch_x, n_samples=1).mean(0)\n",
    "                total_loss += loss / len(loader.dataset)\n",
    "                total += batch_y.size(0)\n",
    "                correct += ((pred.squeeze()>=0.5) == batch_y).sum().item()\n",
    "                param_store = pyro.get_param_store()\n",
    "            accuracy = correct / total\n",
    "            \n",
    "            # Plot current results\n",
    "            plot_decision_boundary(self, X, y, epoch, accuracy, model_type='svi')\n",
    "\n",
    "    def forward(self, x, n_samples=10):\n",
    "        ''' Useful method which enables forward inference with MC sampling'''\n",
    "        res = []\n",
    "        for i in range(n_samples):\n",
    "            t = poutine.trace(self.guide).get_trace(x)\n",
    "            sampled_weight = t.nodes['weight']['value']\n",
    "            sampled_bias = t.nodes['bias']['value']\n",
    "            pred = F.sigmoid(torch.matmul(x, sampled_weight.T) + sampled_bias).squeeze()\n",
    "            res.append(pred)\n",
    "        return torch.stack(res, dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "pyro.clear_param_store()\n",
    "logreg = BayesianLogRegModel(D=2)\n",
    "logreg.infer_parameters(train_dataloader, num_epochs=20, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5: Same as in previous question 2.3, analyze the results showed on plot. This time, what can you say about points far from train distribution?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3 MLP with MCDropout variational inference\n",
    "\n",
    "In the same spirit than II.2, we can leverage the results shown in [[1]](https://arxiv.org/abs/1506.02142) and use MC Dropout variational inference within a MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.6: Implement MLP with dropout model by completing code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Code MLP with 1 hidden layer and a dropout layer. Be careful, the dropout layer should be also activated \n",
    "#during test time.\n",
    "#(Hint: we may want to look out at F.dropout())\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Pytorch MLP for binary classification model with an added dropout layer\"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.type(torch.FloatTensor)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn = MLP(hidden_size=20)\n",
    "bnn.train()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_bnn = torch.optim.SGD(bnn.parameters(), lr=0.01, momentum=0.90, nesterov=True, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.7:  Train MLP with gradient descent for 200 epochs in code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Train MLP for 200 epochs\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "# To Do here\n",
    "\n",
    "\n",
    "# For plotting and showing learning process at each epoch, uncomment line below\n",
    "#if (epoch+1)%10==0:\n",
    "#    plot_decision_boundary(bnn, X, y, epoch, ((output.squeeze()>=0.5) == y).float().mean(), \n",
    "#                           nbh=4, model_type='mcdropout')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.8: Again, analyze the results showed on plot. What is the benefit of MC Dropout variational inference over Bayesian Logistic Regression with variational inference?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: MC Dropout on MNIST\n",
    "\n",
    "Finally, we extend this work on more real-world dataset as MNIST. By appling MC Dropout variational inference method, we're interested to obtain an uncertainty measure which can be use to spot the most uncertain images in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision.datasets.mnist import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), \n",
    "                                            torchvision.transforms.Normalize((0.1307,),(0.3081,))])\n",
    "mnist = MNIST(root='images', transform=transform, download=True)\n",
    "train_set, test_set = random_split(mnist, lengths=(50_000, 10_000))\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some images\n",
    "images, labels = next(iter(train_loader))\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4)\n",
    "for i, (image, label) in enumerate(zip(images, labels)):\n",
    "    if i >= 16:\n",
    "        break\n",
    "    axes[i // 4][i % 4].imshow(images[i][0], cmap='gray')\n",
    "    axes[i // 4][i % 4].set_title(f\"{label}\")\n",
    "    axes[i // 4][i % 4].set_xticks([])\n",
    "    axes[i // 4][i % 4].set_yticks([])\n",
    "fig.set_size_inches(4, 4)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1: Code a LeNet3-style neural network in the following code**\n",
    "\n",
    "(LeNet3 consists of a first convolutional layer with out_features=32, a second convolutional layer with out_features=64, both equipped with a kernel size of 3. Then, apply a Maxpooling of size 2. After flattening, add 2 linear layers whose hidden size is 128. Dropout layers are located before each one of them, first one with p=0.25, second one p=0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Code a LeNet3-style neural network. Be careful, the dropout layer should be also activated \n",
    "#during test time.\n",
    "#(Hint: we may want to look out at F.dropout())\n",
    "\n",
    "class LeNet3(nn.Module):\n",
    "    def __init__(self, n_classes=10):\n",
    "        super().__init__()       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet = LeNet3(n_classes=10).cuda()\n",
    "lenet.train()\n",
    "optim_mcdropout = torch.optim.SGD(lenet.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2:  Train LeNet for 20 epochs in code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Train LeNet3 for 20 epochs. You should approach >99% accuracy on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We predict test set using s=100 samples and entropy as uncertainty measure (see next lesson)\n",
    "# Verifie here that your model achieves also >99% accuracy on test set\n",
    "\n",
    "lenet.eval()\n",
    "lenet.training = True\n",
    "pred, uncertainty, label = [], [], []\n",
    "for image, target in test_loader:\n",
    "    outputs = torch.zeros(100, image.shape[0], 10)\n",
    "    for i in range(100):\n",
    "        with torch.no_grad():\n",
    "            outputs[i] = lenet(image.cuda())\n",
    "    output = outputs.mean(0)\n",
    "    probs = F.softmax(output, dim=1)\n",
    "    entropy = (probs * torch.log(probs + 1e-9)).sum(dim=1)  # entropy measure\n",
    "    pred.extend(probs.max(dim=1, keepdim=True)[1])\n",
    "    label.extend(target)\n",
    "    uncertainty.extend(entropy)\n",
    "\n",
    "pred = np.reshape(pred, newshape=(len(pred), -1)).flatten()\n",
    "label = np.reshape(label, newshape=(len(label), -1)).flatten()\n",
    "uncertainty = np.reshape(uncertainty, newshape=(len(uncertainty), -1)).flatten()\n",
    "\n",
    "print(f'Test set accuracy = {(pred == label).sum()/len(pred):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3: Plot the top-9 most uncertain images along with their entropy value, their ground-truth class and prediction class. What do you note when looking at those images?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: plot top-9 most uncertainy images thanks to the 'uncertainty' vector previously computed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
